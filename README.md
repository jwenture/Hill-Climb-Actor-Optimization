# hill_climb_actor_optimization
Gradient Descent policy estimators are slow to converge and are highly susceptible to initializations. I present a different optimization method that utilizes random hill search and Advantage-Actors to quickly find optimal solutions.
